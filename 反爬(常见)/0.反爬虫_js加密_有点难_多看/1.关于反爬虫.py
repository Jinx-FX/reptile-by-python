#  为什么网站要做反爬虫：大部分爬虫对于网站来说 加大了服务器的负担

#  所以网站会做一些措施，来防止爬虫获取网站的信息和资源

#  采取什么样的措施，来分辨正常请求和爬虫程序？
#  网站需要防止误伤：把真实用于当成爬虫程序来处理

#  最常见的反爬方式：检查请求头（User Agent，referer,cookie）

#  js加密：反爬方式中较为难处理的一类

#  js加密的原理：服务器响应给浏览器的js文件，可以动态的生成一些加密参数，
#               浏览器会根据js的计算 得到这些参数，在请求中带入进来
#               如果请求中没有这些参数，那么服务器就任务请求无效

#  爬虫最重要的思路： 数据展示到网页上的流程
