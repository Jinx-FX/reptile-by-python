# 1. 判断请求头来进行反爬
#       user agent用户代理
#       referer   请求来自哪里
#       cookie   也可以用来做访问凭证
#       解决办法：请求头里面添加对应的参数，复制浏览器

# 2. 根据用户行为来进行反爬
#     -  请求频率过高，服务器设置规定时间之内的请求阈值  解决办法：降低请求频率 或者 使用代理
#     -  网页中设置一些陷阱（正常用户访问不到但是爬虫可以访问到）  解决办法：分析网页
#     -  请求间隔太短，返回相同的数据

# 中国裁判文书网，很难爬取，采用瑞书加密，且更新频率很快

#  模仿浏览器（请求的频率，间隔，请求头参数）

#  3.js加密

#  4.字体加密

#  5.登录: 验证码，
#      简单验证码
#      复杂验证码
